{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lrakotoarivony/Micronet_Challenge/blob/main/Project_Model_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM1Yz4abgeAp"
   },
   "source": [
    "# Notebook réalisé par Lucas Rakotoarivony & Jérémie Sicard\n",
    "\n",
    "Ce Notebook présente les différents résultats et travaux que nous avons effectués dans le cadre du Micronet Challenge.  \n",
    "Nous avons choisi de travailler avec l'architecture Densenet.  \n",
    "Il est important de préciser que l'objectif de ce projet n'est pas d'obtenir l'accuracy la plus importante mais le score Micronet le plus faible tout en ayant un modèle avec une accuracy supérieure à 90%. Pour rappel le score de Micronet se base sur deux facteurs, le nombre de paramètres et le nombre de flops (floating points operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMHg7fp3QQP0"
   },
   "source": [
    "# Data & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wYrygCr3QQP7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "from Densenet import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBhdlahUQQP9",
    "outputId": "ac83b715-e6a2-4c30-d277-b8e42078e3fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26284a12e24826bedc46ec0f753f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_classes_cifar10 = 10\n",
    "train_size = 0.8\n",
    "R = 5\n",
    "\n",
    "\n",
    "# Download the entire CIFAR10 dataset\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np \n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Normalization is different when training from scratch and when training using an imagenet pretrained backbone\n",
    "\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "\n",
    "# Data augmentation is needed in order to train from scratch\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "### The data from CIFAR10 will be downloaded in the following dataset\n",
    "rootdir = './data/cifar10'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR10 is sufficiently large so that training a model up to the state of the art performance will take approximately 3 hours on the 1060 GPU available on your machine. \n",
    "\n",
    "\n",
    "def train_validation_split(train_size, num_train_examples):\n",
    "    # obtain training indices that will be used for validation\n",
    "    indices = list(range(num_train_examples))\n",
    "    np.random.shuffle(indices)\n",
    "    idx_split = int(np.floor(train_size * num_train_examples))\n",
    "    train_index, valid_index = indices[:idx_split], indices[idx_split:]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_index)\n",
    "    valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "    return train_sampler,valid_sampler\n",
    "\n",
    "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
    "\n",
    "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
    "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
    "\n",
    "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
    "\n",
    "    all_indices = []\n",
    "    for curclas in range(n_classes):\n",
    "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
    "        indices_curclas = curtargets[0]\n",
    "        indices_subset = indices_curclas[indices_split]\n",
    "        #print(len(indices_subset))\n",
    "        all_indices.append(indices_subset)\n",
    "    all_indices = np.hstack(all_indices)\n",
    "    \n",
    "    return Subset(dataset,indices=all_indices)\n",
    "    \n",
    "\n",
    "\n",
    "### These dataloader are ready to be used to train for scratch \n",
    "cifar10_train= generate_subset(dataset=c10train,n_classes=n_classes_cifar10,reducefactor=R,n_ex_class_init=5000)\n",
    "num_train_examples=len(cifar10_train)\n",
    "train_sampler,valid_sampler=train_validation_split(train_size, num_train_examples)\n",
    "\n",
    "cifar10_test = generate_subset(dataset=c10test,n_classes=n_classes_cifar10,reducefactor=1,n_ex_class_init=1000) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5jquWwc-QQP-"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(c10train,batch_size=64,sampler=train_sampler)\n",
    "validloader = DataLoader(c10train,batch_size=64,sampler=valid_sampler)\n",
    "testloader = DataLoader(c10test,batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kse9bIgDQQQA"
   },
   "source": [
    "# Device & Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bJJAjcDQQQA",
    "outputId": "835eae3d-a5b8-4fee-fa64-4b75b5ea927e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device '+str(device))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUoUD0qZQQQD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_cifar()\n",
    "model.to(device=device)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHWyF4tTg7yT",
    "outputId": "beb5a7e5-3a6a-49ff-8208-a9af36568a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètre de ce modèle : 331226\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Nombre de paramètre de ce modèle : {pytorch_total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRcopO2qgeA4"
   },
   "source": [
    "Voici les paramètres que nous avons utilisé pour entrainer ce modèle from scratch. (très long à exécuter sans GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jaGghNTTQQQE"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-4) \n",
    "scheduler = MultiStepLR(optimizer, milestones=[90, 110], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AlbZJntsobK"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses, train_acc, valid_acc = training(trainloader, validloader, model, criterion, optimizer,120,scheduler,'models\\\\Densenet_from_scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqhRwBtvQQQF"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(range(n_epochs), train_losses)\n",
    "plt.plot(range(n_epochs), valid_losses)\n",
    "\n",
    "plt.legend(['train', 'validation'], prop={'size': 10})\n",
    "plt.title('loss function', size=10)\n",
    "plt.xlabel('epoch', size=10)\n",
    "plt.ylabel('loss value', size=10)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(range(n_epochs), train_acc)\n",
    "plt.plot(range(n_epochs), valid_acc)\n",
    "\n",
    "plt.legend(['train', 'validation'], prop={'size': 10})\n",
    "plt.title('accuracy', size=10)\n",
    "plt.xlabel('epoch', size=10)\n",
    "plt.ylabel('acc value', size=10)\n",
    "plt.savefig(\"Densenet_training_scratch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL1Ey3hKgeA5"
   },
   "source": [
    "Si vous désirez utiliser un modèle déjà entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bew5LaNhaFqr",
    "outputId": "df3dcb22-9b41-4ec4-ffab-7d5d59e39067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(loaded_cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwWTkVCWfBLM",
    "outputId": "4c4c6a7d-3ce5-4026-aa22-d6de235f4f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.337965\n",
      "\n",
      "test accuracy of plane: 92% (921/1000)\n",
      "test accuracy of car: 96% (965/1000)\n",
      "test accuracy of bird: 90% (901/1000)\n",
      "test accuracy of cat: 83% (833/1000)\n",
      "test accuracy of deer: 94% (945/1000)\n",
      "test accuracy of dog: 86% (869/1000)\n",
      "test accuracy of frog: 92% (926/1000)\n",
      "test accuracy of horse: 94% (941/1000)\n",
      "test accuracy of ship: 95% (956/1000)\n",
      "test accuracy of truck: 94% (948/1000)\n",
      "\n",
      "test accuracy (overall): 92.05% (9205/10000)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score flops: 0.06795413525587332 Score Params: 0.02964266390023521\n",
      "Final score: 0.09759679915610853\n"
     ]
    }
   ],
   "source": [
    "flops , params = score(model)\n",
    "print(\"Score flops: {} Score Params: {}\".format(flops,params))\n",
    "print(\"Final score: {}\".format(flops + params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm9hzZnmgeA6"
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de réaliser notre pruning, il est nécessaire d'avoir un modèle entrainé (nous pouvons alors d'utiliser le notre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WEybhgUNgeA6",
    "outputId": "b6008bf4-4ef1-4805-d379-7ab4f7ffd38a"
   },
   "outputs": [],
   "source": [
    "model_pruned = densenet_cifar()\n",
    "model_pruned.to(device=device)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt', map_location=torch.device('cpu'))\n",
    "model_pruned.load_state_dict(loaded_cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.337965\n",
      "\n",
      "test accuracy of plane: 92% (921/1000)\n",
      "test accuracy of car: 96% (965/1000)\n",
      "test accuracy of bird: 90% (901/1000)\n",
      "test accuracy of cat: 83% (833/1000)\n",
      "test accuracy of deer: 94% (945/1000)\n",
      "test accuracy of dog: 86% (869/1000)\n",
      "test accuracy of frog: 92% (926/1000)\n",
      "test accuracy of horse: 94% (941/1000)\n",
      "test accuracy of ship: 95% (956/1000)\n",
      "test accuracy of truck: 94% (948/1000)\n",
      "\n",
      "test accuracy (overall): 92.05% (9205/10000)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model_pruned, testloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd61NjUtgeA7"
   },
   "source": [
    "Nous allons réaliser de l'unstructured pruning de façon itérative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque itération, nous allons pruner de façon globale 20 % des poids du modèle qui ont la plus faible norme L1.\n",
    "Nous allons également réentrainer en utilisant la technique du learning weight rewinding qui diffère nottament d'une technique plus conventionnelle (celle du fine tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous trouverez davantage d'informations sur le lien suivant :\n",
    "https://iclr.cc/virtual_2020/poster_S1gSj0NKvB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons également réentrainer entre chaque itération de pruning en utilisant la technique du learning rate rewinding. \n",
    "En considérant le modèle actuel à un instant T, le réentrainer en utilisant la technique du learning rate rewinding consiste à utiliser le modèle pruné actuel (instant T), les poids actuels associés (instant T) mais en utilisant le learning rate des X epochs précédentes (instant T-X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas d'étude, nous avons choisi de réentrainer le modèle après chaque phase de pruning de 60 epochs.\n",
    "Ainsi nous utilisons :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lr = 0.1 pour les epochs de [1 à 29] ;\n",
    "Lr = 0.01 pour les epochs de [30 à 49] ;\n",
    "Lr = 0.001 pour les epochs de [50 à 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous avons également choisi de faire 7 étapes de pruning afin d'obtenir un ratio de compression égal environ à 4.76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-4) \n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 50], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune=[]\n",
    "for name, module in model_pruned.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) :\n",
    "        parameters_to_prune.append((module,'weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.336704\n",
      "\n",
      "test accuracy of plane: 92% (924/1000)\n",
      "test accuracy of car: 96% (960/1000)\n",
      "test accuracy of bird: 89% (897/1000)\n",
      "test accuracy of cat: 81% (818/1000)\n",
      "test accuracy of deer: 94% (943/1000)\n",
      "test accuracy of dog: 87% (872/1000)\n",
      "test accuracy of frog: 93% (930/1000)\n",
      "test accuracy of horse: 94% (949/1000)\n",
      "test accuracy of ship: 95% (955/1000)\n",
      "test accuracy of truck: 94% (949/1000)\n",
      "\n",
      "test accuracy (overall): 91.97% (9197/10000)\n",
      "test Loss: 0.373786\n",
      "\n",
      "test accuracy of plane: 91% (911/1000)\n",
      "test accuracy of car: 96% (964/1000)\n",
      "test accuracy of bird: 90% (903/1000)\n",
      "test accuracy of cat: 82% (828/1000)\n",
      "test accuracy of deer: 94% (941/1000)\n",
      "test accuracy of dog: 88% (881/1000)\n",
      "test accuracy of frog: 92% (920/1000)\n",
      "test accuracy of horse: 89% (893/1000)\n",
      "test accuracy of ship: 95% (955/1000)\n",
      "test accuracy of truck: 93% (933/1000)\n",
      "\n",
      "test accuracy (overall): 91.29% (9129/10000)\n",
      "test Loss: 0.388742\n",
      "\n",
      "test accuracy of plane: 93% (931/1000)\n",
      "test accuracy of car: 96% (960/1000)\n",
      "test accuracy of bird: 89% (899/1000)\n",
      "test accuracy of cat: 82% (829/1000)\n",
      "test accuracy of deer: 93% (939/1000)\n",
      "test accuracy of dog: 84% (845/1000)\n",
      "test accuracy of frog: 85% (857/1000)\n",
      "test accuracy of horse: 94% (944/1000)\n",
      "test accuracy of ship: 93% (935/1000)\n",
      "test accuracy of truck: 94% (946/1000)\n",
      "\n",
      "test accuracy (overall): 90.85% (9085/10000)\n",
      "test Loss: 0.482012\n",
      "\n",
      "test accuracy of plane: 89% (890/1000)\n",
      "test accuracy of car: 92% (925/1000)\n",
      "test accuracy of bird: 93% (931/1000)\n",
      "test accuracy of cat: 72% (728/1000)\n",
      "test accuracy of deer: 92% (929/1000)\n",
      "test accuracy of dog: 88% (880/1000)\n",
      "test accuracy of frog: 87% (878/1000)\n",
      "test accuracy of horse: 87% (871/1000)\n",
      "test accuracy of ship: 86% (861/1000)\n",
      "test accuracy of truck: 90% (906/1000)\n",
      "\n",
      "test accuracy (overall): 87.99% (8799/10000)\n",
      "test Loss: 0.587265\n",
      "\n",
      "test accuracy of plane: 88% (883/1000)\n",
      "test accuracy of car: 81% (814/1000)\n",
      "test accuracy of bird: 82% (821/1000)\n",
      "test accuracy of cat: 62% (628/1000)\n",
      "test accuracy of deer: 88% (882/1000)\n",
      "test accuracy of dog: 88% (887/1000)\n",
      "test accuracy of frog: 84% (843/1000)\n",
      "test accuracy of horse: 90% (906/1000)\n",
      "test accuracy of ship: 84% (846/1000)\n",
      "test accuracy of truck: 89% (892/1000)\n",
      "\n",
      "test accuracy (overall): 84.02% (8402/10000)\n",
      "test Loss: 1.016551\n",
      "\n",
      "test accuracy of plane: 83% (833/1000)\n",
      "test accuracy of car: 49% (490/1000)\n",
      "test accuracy of bird: 93% (930/1000)\n",
      "test accuracy of cat: 79% (794/1000)\n",
      "test accuracy of deer: 85% (857/1000)\n",
      "test accuracy of dog: 54% (540/1000)\n",
      "test accuracy of frog: 74% (748/1000)\n",
      "test accuracy of horse: 76% (769/1000)\n",
      "test accuracy of ship: 66% (664/1000)\n",
      "test accuracy of truck: 81% (817/1000)\n",
      "\n",
      "test accuracy (overall): 74.42% (7442/10000)\n",
      "test Loss: 1.295276\n",
      "\n",
      "test accuracy of plane: 54% (540/1000)\n",
      "test accuracy of car: 25% (252/1000)\n",
      "test accuracy of bird: 42% (421/1000)\n",
      "test accuracy of cat: 84% (847/1000)\n",
      "test accuracy of deer: 89% (897/1000)\n",
      "test accuracy of dog: 54% (542/1000)\n",
      "test accuracy of frog: 67% (677/1000)\n",
      "test accuracy of horse: 82% (823/1000)\n",
      "test accuracy of ship: 84% (844/1000)\n",
      "test accuracy of truck: 85% (859/1000)\n",
      "\n",
      "test accuracy (overall): 67.02% (6702/10000)\n"
     ]
    }
   ],
   "source": [
    "steps_pruning = 7\n",
    "for steps in range (steps_pruning):\n",
    "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "    evaluation(model_pruned, testloader, criterion)\n",
    "    training_pruning(trainloader,model_pruned, criterion, optimizer,60,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy au bout de la 7ème étape de pruning est égale à :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.656300\n",
      "\n",
      "test accuracy of plane: 74% (743/1000)\n",
      "test accuracy of car: 78% (786/1000)\n",
      "test accuracy of bird: 79% (795/1000)\n",
      "test accuracy of cat: 76% (761/1000)\n",
      "test accuracy of deer: 88% (880/1000)\n",
      "test accuracy of dog: 73% (731/1000)\n",
      "test accuracy of frog: 87% (874/1000)\n",
      "test accuracy of horse: 80% (804/1000)\n",
      "test accuracy of ship: 90% (903/1000)\n",
      "test accuracy of truck: 88% (886/1000)\n",
      "\n",
      "test accuracy (overall): 81.63% (8163/10000)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model_pruned, testloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le pruning est bien réalisé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1t: 25.93%\n",
      "Sparsity in dense1.0.conv1t: 64.84%\n",
      "Sparsity in dense1.0.conv2t: 79.17%\n",
      "Sparsity in dense1.1.conv1t: 69.40%\n",
      "Sparsity in dense1.1.conv2t: 71.74%\n",
      "Sparsity in dense1.2.conv1t: 70.02%\n",
      "Sparsity in dense1.2.conv2t: 74.00%\n",
      "Sparsity in dense1.3.conv1t: 80.78%\n",
      "Sparsity in dense1.3.conv2t: 77.13%\n",
      "Sparsity in dense1.4.conv1t: 79.49%\n",
      "Sparsity in dense1.4.conv2t: 76.09%\n",
      "Sparsity in dense1.5.conv1t: 73.72%\n",
      "Sparsity in dense1.5.conv2t: 73.39%\n",
      "Sparsity in trans1.convt: 48.34%\n",
      "Sparsity in dense2.0.conv1t: 69.82%\n",
      "Sparsity in dense2.0.conv2t: 72.70%\n",
      "Sparsity in dense2.1.conv1t: 76.41%\n",
      "Sparsity in dense2.1.conv2t: 77.65%\n",
      "Sparsity in dense2.2.conv1t: 75.13%\n",
      "Sparsity in dense2.2.conv2t: 70.18%\n",
      "Sparsity in dense2.3.conv1t: 73.66%\n",
      "Sparsity in dense2.3.conv2t: 73.09%\n",
      "Sparsity in dense2.4.conv1t: 72.71%\n",
      "Sparsity in dense2.4.conv2t: 70.31%\n",
      "Sparsity in dense2.5.conv1t: 73.74%\n",
      "Sparsity in dense2.5.conv2t: 69.14%\n",
      "Sparsity in dense2.6.conv1t: 70.12%\n",
      "Sparsity in dense2.6.conv2t: 69.70%\n",
      "Sparsity in dense2.7.conv1t: 73.65%\n",
      "Sparsity in dense2.7.conv2t: 69.75%\n",
      "Sparsity in dense2.8.conv1t: 74.87%\n",
      "Sparsity in dense2.8.conv2t: 70.40%\n",
      "Sparsity in dense2.9.conv1t: 79.72%\n",
      "Sparsity in dense2.9.conv2t: 78.82%\n",
      "Sparsity in trans2.convt: 48.95%\n",
      "Sparsity in dense3.0.conv1t: 82.14%\n",
      "Sparsity in dense3.0.conv2t: 78.52%\n",
      "Sparsity in dense3.1.conv1t: 82.52%\n",
      "Sparsity in dense3.1.conv2t: 73.87%\n",
      "Sparsity in dense3.2.conv1t: 82.03%\n",
      "Sparsity in dense3.2.conv2t: 77.00%\n",
      "Sparsity in dense3.3.conv1t: 78.87%\n",
      "Sparsity in dense3.3.conv2t: 74.44%\n",
      "Sparsity in dense3.4.conv1t: 74.89%\n",
      "Sparsity in dense3.4.conv2t: 70.01%\n",
      "Sparsity in dense3.5.conv1t: 78.09%\n",
      "Sparsity in dense3.5.conv2t: 71.57%\n",
      "Sparsity in dense3.6.conv1t: 80.86%\n",
      "Sparsity in dense3.6.conv2t: 74.35%\n",
      "Sparsity in dense3.7.conv1t: 81.19%\n",
      "Sparsity in dense3.7.conv2t: 73.87%\n",
      "Sparsity in dense3.8.conv1t: 82.14%\n",
      "Sparsity in dense3.8.conv2t: 73.35%\n",
      "Sparsity in dense3.9.conv1t: 80.35%\n",
      "Sparsity in dense3.9.conv2t: 73.00%\n",
      "Sparsity in dense3.10.conv1t: 78.54%\n",
      "Sparsity in dense3.10.conv2t: 72.05%\n",
      "Sparsity in dense3.11.conv1t: 78.93%\n",
      "Sparsity in dense3.11.conv2t: 67.71%\n",
      "Sparsity in dense3.12.conv1t: 78.50%\n",
      "Sparsity in dense3.12.conv2t: 72.44%\n",
      "Sparsity in dense3.13.conv1t: 79.12%\n",
      "Sparsity in dense3.13.conv2t: 71.92%\n",
      "Sparsity in dense3.14.conv1t: 79.41%\n",
      "Sparsity in dense3.14.conv2t: 70.36%\n",
      "Sparsity in dense3.15.conv1t: 79.78%\n",
      "Sparsity in dense3.15.conv2t: 70.23%\n",
      "Sparsity in dense3.16.conv1t: 80.32%\n",
      "Sparsity in dense3.16.conv2t: 72.35%\n",
      "Sparsity in dense3.17.conv1t: 79.04%\n",
      "Sparsity in dense3.17.conv2t: 70.57%\n",
      "Sparsity in dense3.18.conv1t: 79.89%\n",
      "Sparsity in dense3.18.conv2t: 71.88%\n",
      "Sparsity in dense3.19.conv1t: 79.51%\n",
      "Sparsity in dense3.19.conv2t: 71.01%\n",
      "Sparsity in trans3.convt: 74.22%\n",
      "Sparsity in dense4.0.conv1t: 90.31%\n",
      "Sparsity in dense4.0.conv2t: 85.59%\n",
      "Sparsity in dense4.1.conv1t: 91.59%\n",
      "Sparsity in dense4.1.conv2t: 83.68%\n",
      "Sparsity in dense4.2.conv1t: 90.57%\n",
      "Sparsity in dense4.2.conv2t: 85.63%\n",
      "Sparsity in dense4.3.conv1t: 93.28%\n",
      "Sparsity in dense4.3.conv2t: 88.72%\n",
      "Sparsity in dense4.4.conv1t: 90.67%\n",
      "Sparsity in dense4.4.conv2t: 90.19%\n",
      "Sparsity in dense4.5.conv1t: 94.28%\n",
      "Sparsity in dense4.5.conv2t: 92.93%\n",
      "Sparsity in dense4.6.conv1t: 91.87%\n",
      "Sparsity in dense4.6.conv2t: 89.80%\n",
      "Sparsity in dense4.7.conv1t: 90.42%\n",
      "Sparsity in dense4.7.conv2t: 87.50%\n",
      "Sparsity in dense4.8.conv1t: 91.62%\n",
      "Sparsity in dense4.8.conv2t: 90.49%\n",
      "Sparsity in dense4.9.conv1t: 91.75%\n",
      "Sparsity in dense4.9.conv2t: 92.10%\n",
      "Sparsity in dense4.10.conv1t: 93.09%\n",
      "Sparsity in dense4.10.conv2t: 93.14%\n",
      "Sparsity in dense4.11.conv1t: 93.53%\n",
      "Sparsity in dense4.11.conv2t: 93.36%\n",
      "Sparsity in lineart: 44.61%\n",
      "Global sparsity: 79.03%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'79.0286865234375'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(model_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc8A4ugcgeA7"
   },
   "source": [
    "Si vous voulez travailler avec notre modèle déjà pruné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune=[]\n",
    "for name, module in model_pruned.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) :\n",
    "        parameters_to_prune.append((module,'weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "C9Fh1sQLgeA8",
    "outputId": "ea359b59-9eb5-4a69-ba8c-bb40944961fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt', map_location=torch.device('cpu'))\n",
    "model_pruned.load_state_dict(loaded_cpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmnCTg6qgeA8"
   },
   "source": [
    "Il est nécessaire d'exécuter un forward afin que le modèle soit pruné de manière correcte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "yOdTZYw_geA9",
    "outputId": "826c356a-ec70-453c-d753-53d5e86f4ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.304136\n",
      "\n",
      "test accuracy of plane: 93% (939/1000)\n",
      "test accuracy of car: 97% (974/1000)\n",
      "test accuracy of bird: 90% (903/1000)\n",
      "test accuracy of cat: 84% (843/1000)\n",
      "test accuracy of deer: 95% (950/1000)\n",
      "test accuracy of dog: 88% (881/1000)\n",
      "test accuracy of frog: 94% (944/1000)\n",
      "test accuracy of horse: 93% (935/1000)\n",
      "test accuracy of ship: 94% (949/1000)\n",
      "test accuracy of truck: 94% (947/1000)\n",
      "\n",
      "test accuracy (overall): 92.65% (9265/10000)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model_pruned, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tI2vBSyhgeA-",
    "outputId": "bcb5ebb4-3029-41aa-b7b0-fe20aa5a9670",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1t: 26.85%\n",
      "Sparsity in dense1.0.conv1t: 74.80%\n",
      "Sparsity in dense1.0.conv2t: 76.65%\n",
      "Sparsity in dense1.1.conv1t: 71.61%\n",
      "Sparsity in dense1.1.conv2t: 69.97%\n",
      "Sparsity in dense1.2.conv1t: 76.37%\n",
      "Sparsity in dense1.2.conv2t: 77.17%\n",
      "Sparsity in dense1.3.conv1t: 83.52%\n",
      "Sparsity in dense1.3.conv2t: 77.86%\n",
      "Sparsity in dense1.4.conv1t: 78.78%\n",
      "Sparsity in dense1.4.conv2t: 73.52%\n",
      "Sparsity in dense1.5.conv1t: 77.12%\n",
      "Sparsity in dense1.5.conv2t: 74.78%\n",
      "Sparsity in trans1.convt: 56.93%\n",
      "Sparsity in dense2.0.conv1t: 71.29%\n",
      "Sparsity in dense2.0.conv2t: 72.83%\n",
      "Sparsity in dense2.1.conv1t: 78.52%\n",
      "Sparsity in dense2.1.conv2t: 74.78%\n",
      "Sparsity in dense2.2.conv1t: 77.28%\n",
      "Sparsity in dense2.2.conv2t: 70.01%\n",
      "Sparsity in dense2.3.conv1t: 79.63%\n",
      "Sparsity in dense2.3.conv2t: 71.66%\n",
      "Sparsity in dense2.4.conv1t: 76.32%\n",
      "Sparsity in dense2.4.conv2t: 71.74%\n",
      "Sparsity in dense2.5.conv1t: 76.56%\n",
      "Sparsity in dense2.5.conv2t: 70.36%\n",
      "Sparsity in dense2.6.conv1t: 71.68%\n",
      "Sparsity in dense2.6.conv2t: 70.27%\n",
      "Sparsity in dense2.7.conv1t: 77.70%\n",
      "Sparsity in dense2.7.conv2t: 73.00%\n",
      "Sparsity in dense2.8.conv1t: 76.89%\n",
      "Sparsity in dense2.8.conv2t: 70.92%\n",
      "Sparsity in dense2.9.conv1t: 79.78%\n",
      "Sparsity in dense2.9.conv2t: 76.91%\n",
      "Sparsity in trans2.convt: 55.95%\n",
      "Sparsity in dense3.0.conv1t: 82.42%\n",
      "Sparsity in dense3.0.conv2t: 77.95%\n",
      "Sparsity in dense3.1.conv1t: 85.35%\n",
      "Sparsity in dense3.1.conv2t: 76.74%\n",
      "Sparsity in dense3.2.conv1t: 85.68%\n",
      "Sparsity in dense3.2.conv2t: 78.43%\n",
      "Sparsity in dense3.3.conv1t: 81.91%\n",
      "Sparsity in dense3.3.conv2t: 77.13%\n",
      "Sparsity in dense3.4.conv1t: 80.43%\n",
      "Sparsity in dense3.4.conv2t: 73.91%\n",
      "Sparsity in dense3.5.conv1t: 81.41%\n",
      "Sparsity in dense3.5.conv2t: 74.57%\n",
      "Sparsity in dense3.6.conv1t: 83.38%\n",
      "Sparsity in dense3.6.conv2t: 76.87%\n",
      "Sparsity in dense3.7.conv1t: 83.59%\n",
      "Sparsity in dense3.7.conv2t: 75.52%\n",
      "Sparsity in dense3.8.conv1t: 83.54%\n",
      "Sparsity in dense3.8.conv2t: 74.96%\n",
      "Sparsity in dense3.9.conv1t: 83.13%\n",
      "Sparsity in dense3.9.conv2t: 76.26%\n",
      "Sparsity in dense3.10.conv1t: 81.07%\n",
      "Sparsity in dense3.10.conv2t: 75.78%\n",
      "Sparsity in dense3.11.conv1t: 82.60%\n",
      "Sparsity in dense3.11.conv2t: 70.57%\n",
      "Sparsity in dense3.12.conv1t: 80.10%\n",
      "Sparsity in dense3.12.conv2t: 74.96%\n",
      "Sparsity in dense3.13.conv1t: 81.88%\n",
      "Sparsity in dense3.13.conv2t: 74.48%\n",
      "Sparsity in dense3.14.conv1t: 80.71%\n",
      "Sparsity in dense3.14.conv2t: 73.05%\n",
      "Sparsity in dense3.15.conv1t: 81.27%\n",
      "Sparsity in dense3.15.conv2t: 70.27%\n",
      "Sparsity in dense3.16.conv1t: 79.99%\n",
      "Sparsity in dense3.16.conv2t: 72.27%\n",
      "Sparsity in dense3.17.conv1t: 80.58%\n",
      "Sparsity in dense3.17.conv2t: 71.01%\n",
      "Sparsity in dense3.18.conv1t: 80.02%\n",
      "Sparsity in dense3.18.conv2t: 71.48%\n",
      "Sparsity in dense3.19.conv1t: 80.11%\n",
      "Sparsity in dense3.19.conv2t: 70.62%\n",
      "Sparsity in trans3.convt: 74.43%\n",
      "Sparsity in dense4.0.conv1t: 89.06%\n",
      "Sparsity in dense4.0.conv2t: 80.21%\n",
      "Sparsity in dense4.1.conv1t: 87.77%\n",
      "Sparsity in dense4.1.conv2t: 82.38%\n",
      "Sparsity in dense4.2.conv1t: 89.44%\n",
      "Sparsity in dense4.2.conv2t: 83.03%\n",
      "Sparsity in dense4.3.conv1t: 90.10%\n",
      "Sparsity in dense4.3.conv2t: 83.25%\n",
      "Sparsity in dense4.4.conv1t: 85.47%\n",
      "Sparsity in dense4.4.conv2t: 82.20%\n",
      "Sparsity in dense4.5.conv1t: 89.70%\n",
      "Sparsity in dense4.5.conv2t: 86.15%\n",
      "Sparsity in dense4.6.conv1t: 87.20%\n",
      "Sparsity in dense4.6.conv2t: 82.29%\n",
      "Sparsity in dense4.7.conv1t: 86.47%\n",
      "Sparsity in dense4.7.conv2t: 80.77%\n",
      "Sparsity in dense4.8.conv1t: 87.45%\n",
      "Sparsity in dense4.8.conv2t: 82.25%\n",
      "Sparsity in dense4.9.conv1t: 86.34%\n",
      "Sparsity in dense4.9.conv2t: 82.42%\n",
      "Sparsity in dense4.10.conv1t: 88.16%\n",
      "Sparsity in dense4.10.conv2t: 84.11%\n",
      "Sparsity in dense4.11.conv1t: 88.04%\n",
      "Sparsity in dense4.11.conv2t: 82.81%\n",
      "Sparsity in lineart: 67.50%\n",
      "Global sparsity: 79.03%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'79.0286865234375'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(model_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "b_daCLkUgeA_",
    "outputId": "56a38f6b-0321-4474-8b1c-482679e0c5db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-9.8413e-01, -1.4156e+00, -6.8998e-01],\n",
      "          [ 2.9501e-01,  2.5201e-01, -2.1341e-01],\n",
      "          [ 1.1402e+00,  1.6780e+00,  2.3051e-01]],\n",
      "\n",
      "         [[ 2.8061e-01,  1.4054e-01,  2.8852e-01],\n",
      "          [-1.0003e-01, -1.5904e-01,  0.0000e+00],\n",
      "          [-3.6462e-01, -1.8441e-01, -1.7617e-01]],\n",
      "\n",
      "         [[ 7.9544e-01,  8.7149e-01,  5.3565e-01],\n",
      "          [-4.3006e-02, -2.3282e-01,  2.8181e-01],\n",
      "          [-9.3424e-01, -1.1104e+00, -2.4479e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4305e-01,  1.0020e+00,  5.2035e-01],\n",
      "          [ 6.1827e-01,  1.2533e+00,  6.1121e-01],\n",
      "          [ 4.5072e-01,  9.9235e-01, -0.0000e+00]],\n",
      "\n",
      "         [[-3.3692e-01, -5.0674e-01, -2.5158e-01],\n",
      "          [-4.8995e-01, -6.7966e-01, -4.1674e-01],\n",
      "          [-2.7760e-01, -3.8856e-01, -2.1192e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -4.3839e-01, -7.1969e-02],\n",
      "          [-2.5402e-01, -6.4782e-01, -1.4823e-01],\n",
      "          [-2.6778e-01, -5.2079e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00, -4.7254e-01, -0.0000e+00],\n",
      "          [-3.8899e-01, -7.3799e-01,  0.0000e+00],\n",
      "          [-2.4230e-01, -5.5155e-01, -0.0000e+00]],\n",
      "\n",
      "         [[-0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "          [-0.0000e+00, -1.7516e-01, -1.5319e-01]],\n",
      "\n",
      "         [[-0.0000e+00,  6.0962e-01, -0.0000e+00],\n",
      "          [ 3.5902e-01,  9.3090e-01,  1.7458e-01],\n",
      "          [ 0.0000e+00,  7.0812e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  4.4110e-01, -0.0000e+00],\n",
      "          [ 3.3788e-01, -4.7360e-01,  1.6912e-01],\n",
      "          [ 2.8527e-01, -8.1426e-01,  4.1107e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [ 4.7869e-01, -1.0405e+00,  3.2554e-01],\n",
      "          [ 4.1865e-01, -9.4973e-01,  5.0380e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [-0.0000e+00, -3.6631e-01,  7.8856e-02],\n",
      "          [ 3.6756e-01, -3.8304e-01,  3.3946e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.6982e-01, -5.0541e-01, -2.3014e-01],\n",
      "          [-1.2593e-01, -1.7216e-01, -0.0000e+00],\n",
      "          [ 4.3483e-01,  5.2029e-01,  3.8644e-01]],\n",
      "\n",
      "         [[-5.6961e-01, -6.1926e-01, -3.9938e-01],\n",
      "          [-0.0000e+00, -1.4251e-01, -1.2758e-01],\n",
      "          [ 5.1014e-01,  6.8602e-01,  5.4614e-01]],\n",
      "\n",
      "         [[-4.6835e-01, -5.0148e-01, -2.4086e-01],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [ 3.4259e-01,  5.1157e-01,  4.6065e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1324e-01, -2.9141e-01,  2.0902e-01],\n",
      "          [-2.8571e-01, -1.0938e+00, -3.6303e-01],\n",
      "          [-0.0000e+00, -1.0641e-01,  1.9788e-01]],\n",
      "\n",
      "         [[ 2.6431e-01, -4.0834e-01,  1.5142e-01],\n",
      "          [-3.9989e-01, -1.1577e+00, -3.2436e-01],\n",
      "          [ 1.8260e-01, -2.8917e-01,  1.3830e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -1.9971e-01,  1.5384e-01],\n",
      "          [ 0.0000e+00, -7.1164e-01, -1.1820e-01],\n",
      "          [ 0.0000e+00,  1.4409e-02,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00, -3.1543e-01, -0.0000e+00],\n",
      "          [-2.0661e-01,  1.1168e+00, -1.5757e-01],\n",
      "          [ 0.0000e+00, -6.1981e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-0.0000e+00,  1.6292e+00, -1.5669e-01],\n",
      "          [ 0.0000e+00, -1.1785e+00, -0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00, -3.7768e-01, -0.0000e+00],\n",
      "          [-2.1569e-01,  7.9693e-01, -1.7729e-01],\n",
      "          [-0.0000e+00, -2.7482e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [ 0.0000e+00,  1.2448e-02,  0.0000e+00],\n",
      "          [-0.0000e+00,  4.0000e-04, -0.0000e+00]],\n",
      "\n",
      "         [[-1.2301e-01, -2.8946e-01, -2.0428e-01],\n",
      "          [-3.8193e-01, -5.0708e-01, -3.6042e-01],\n",
      "          [-3.4098e-01, -4.6607e-01, -3.2657e-01]],\n",
      "\n",
      "         [[ 0.0000e+00,  4.1621e-01,  1.6965e-01],\n",
      "          [ 2.6947e-01,  6.0528e-01,  3.3295e-01],\n",
      "          [ 2.2254e-01,  4.5765e-01,  1.8941e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  6.9632e-01,  9.7630e-02],\n",
      "          [ 5.1361e-02,  5.7410e-01,  1.3419e-01],\n",
      "          [ 0.0000e+00,  2.9380e-01, -1.3187e-01]],\n",
      "\n",
      "         [[-2.7805e-01, -5.3869e-01, -2.2904e-01],\n",
      "          [-5.9110e-01, -1.0547e+00, -5.2018e-01],\n",
      "          [-1.1395e-01, -4.5911e-01, -0.0000e+00]],\n",
      "\n",
      "         [[ 2.6841e-01, -0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  1.3813e-01,  0.0000e+00],\n",
      "          [ 2.4214e-01,  0.0000e+00,  2.4853e-01]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00, -3.7970e-01, -0.0000e+00],\n",
      "          [-3.4311e-01, -7.3491e-01, -4.0605e-01],\n",
      "          [ 0.0000e+00, -4.7254e-01, -0.0000e+00]],\n",
      "\n",
      "         [[-0.0000e+00,  7.6190e-01,  2.6392e-01],\n",
      "          [ 5.9488e-01,  1.0001e+00,  5.4895e-01],\n",
      "          [ 2.8909e-01,  6.4266e-01,  2.6684e-01]],\n",
      "\n",
      "         [[-7.8513e-02, -4.7072e-01, -2.1646e-01],\n",
      "          [-5.1391e-01, -9.9281e-01, -5.8917e-01],\n",
      "          [-2.5533e-01, -0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3364e-01,  3.6325e-01,  1.5123e-01],\n",
      "          [ 4.3976e-01,  6.6283e-01,  3.7862e-01],\n",
      "          [ 2.0271e-01,  3.8074e-01,  1.3587e-01]],\n",
      "\n",
      "         [[-3.3478e-01, -8.1483e-01, -3.3363e-01],\n",
      "          [-4.7842e-01, -1.0189e+00, -5.2572e-01],\n",
      "          [-2.2282e-01, -6.5238e-01, -2.8692e-01]],\n",
      "\n",
      "         [[ 1.3697e-01,  2.4855e-01,  1.8996e-01],\n",
      "          [ 4.3519e-01,  5.9202e-01,  5.0296e-01],\n",
      "          [ 2.0306e-01,  3.6001e-01,  2.4826e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6956e-01,  0.0000e+00, -0.0000e+00],\n",
      "          [ 8.1218e-01, -2.9444e-01, -5.1109e-01],\n",
      "          [ 4.1659e-01, -3.4501e-01, -2.5072e-01]],\n",
      "\n",
      "         [[ 3.1337e-01, -0.0000e+00, -2.8899e-01],\n",
      "          [ 9.8871e-01, -3.7623e-01, -5.9153e-01],\n",
      "          [ 6.1956e-01, -2.8930e-01, -2.6043e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -1.8084e-01, -0.0000e+00],\n",
      "          [ 6.4552e-01, -2.9018e-01, -4.6191e-01],\n",
      "          [ 3.8976e-01, -1.9373e-01, -1.7750e-01]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00, -2.0706e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00, -3.8073e-01, -3.1366e-01],\n",
      "          [ 3.5945e-01,  2.4533e-01,  1.9286e-01]],\n",
      "\n",
      "         [[-0.0000e+00, -4.8968e-01, -0.0000e+00],\n",
      "          [-0.0000e+00, -5.3423e-01, -3.0764e-01],\n",
      "          [ 5.9013e-01,  5.1361e-01,  4.5835e-01]],\n",
      "\n",
      "         [[ 0.0000e+00, -3.0440e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00, -4.4996e-01, -2.3117e-01],\n",
      "          [ 2.6832e-01,  2.0487e-01,  2.7745e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "          [-8.1087e-01,  0.0000e+00,  8.6806e-01],\n",
      "          [-4.0330e-01, -0.0000e+00,  3.1898e-01]],\n",
      "\n",
      "         [[-4.9240e-01,  0.0000e+00,  5.5338e-01],\n",
      "          [-1.1292e+00, -0.0000e+00,  1.1279e+00],\n",
      "          [-5.8523e-01, -0.0000e+00,  6.4221e-01]],\n",
      "\n",
      "         [[-2.3918e-01, -0.0000e+00,  0.0000e+00],\n",
      "          [-8.8233e-01, -0.0000e+00,  8.6712e-01],\n",
      "          [-4.2717e-01,  0.0000e+00,  4.6224e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4984e-01, -0.0000e+00, -0.0000e+00],\n",
      "          [ 1.3402e-01,  1.4740e-01,  0.0000e+00],\n",
      "          [ 1.1590e-01,  2.1890e-01,  9.2538e-02]],\n",
      "\n",
      "         [[ 0.0000e+00,  5.2597e-02,  0.0000e+00],\n",
      "          [ 2.7897e-01,  4.2815e-01,  3.8612e-01],\n",
      "          [ 3.9241e-02,  2.1514e-01,  1.6046e-01]],\n",
      "\n",
      "         [[ 1.1231e-01,  0.0000e+00,  0.0000e+00],\n",
      "          [ 2.0778e-01, -0.0000e+00,  2.1491e-01],\n",
      "          [-1.0233e-01, -2.1000e-01, -5.9420e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0058e-01,  6.2361e-01,  4.3968e-01],\n",
      "          [-0.0000e+00, -7.9785e-02,  0.0000e+00],\n",
      "          [-4.1013e-01, -6.6082e-01, -2.5601e-01]],\n",
      "\n",
      "         [[ 5.2743e-01,  7.0398e-01,  6.0337e-01],\n",
      "          [ 0.0000e+00, -2.0621e-01,  0.0000e+00],\n",
      "          [-4.6769e-01, -7.5969e-01, -4.2095e-01]],\n",
      "\n",
      "         [[ 2.6357e-01,  4.1237e-01,  2.5023e-01],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [-2.8227e-01, -4.3426e-01, -3.1690e-01]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model_pruned.conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onH0khiNgeA_"
   },
   "source": [
    "On voit que notre accuracy a augmenté (environ 0.6%), de plus le nombre de paramètres réduit à zéro est assez important.  \n",
    "De ce fait notre score Micronet associé aux nombres de paramètres a diminué de manière importante ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En exécutant cette cellule, nous perdons le mask associé au pruning, mais cela nous permet de pouvoir calculer le score micronet\n",
    "for name, module in model_pruned.named_modules():\n",
    "# prune X % of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score flops: 0.06795413525587332 Score Params: 0.007276917533816564\n",
      "Final score: 0.07523105278968989\n"
     ]
    }
   ],
   "source": [
    "flops , params = score(model_pruned)\n",
    "print(\"Score flops: {} Score Params: {}\".format(flops,params))\n",
    "print(\"Final score: {}\".format(flops + params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w38B9iUbq0eQ"
   },
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTHdwmwCgeBA"
   },
   "source": [
    "Nous allons tout d'abord présenter deux méthodes de Quantization que nous avons utilisé (Binary Connect et BWN). Vous trouverez plus d'informations sur les liens suivants.  \n",
    "Binary Connect : https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html$  \n",
    "BWN : https://link.springer.com/chapter/10.1007/978-3-319-46493-0_32  \n",
    "Ces méthodes sont intéressantes cependant leur utilisation ne nous permettaient pas d'obtenir une accuracy supérieure à 90% donc nous nous sommes tournés vers une autre méthode.\n",
    "Nous allons quand même montrer un exemple d'utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mcX8Go1ctxQ"
   },
   "outputs": [],
   "source": [
    "modelbc = BC(model)\n",
    "modelbc.model = modelbc.model.to(device)\n",
    "\n",
    "optimizer_bc = torch.optim.SGD(modelbc.model.parameters(),lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "n5HPG0p0cwrP",
    "outputId": "6dfe54f9-d03b-4dd2-f92d-5e36ec58d4d2"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses, train_acc, valid_acc = training_binary(100, trainloader, validloader, modelbc, criterion, optimizer_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o8R6BjBgeBB"
   },
   "outputs": [],
   "source": [
    "evaluation_binary(modelbc,testloader,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIUUZjphgeBB"
   },
   "source": [
    "Nous allons maintenant nous intéresser à une autre méthode de Quantization, l'ApOT Quantization : https://iclr.cc/virtual_2020/poster_BkgXT24tDS.html  \n",
    "Pour résumer, cette méthode nous permet de quantizer les valeurs de paramères sur un n bits (dans notre cas 4 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrPN9yGJrU8x",
    "outputId": "b1b6305f-a44e-46d6-a02b-90f8ebc08504",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_quant = densenet_cifar_quant()\n",
    "model_quant.to(device=device)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKm3vlj-geBC"
   },
   "source": [
    "On peut bien entendu l'entrainer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0jWqjBAcgeBC"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-4) \n",
    "scheduler = MultiStepLR(optimizer, milestones=[90, 110], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp3SdWATgeBC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses, train_acc, valid_acc = training(trainloader, validloader, model_quant, criterion, optimizer,120,scheduler,'models\\\\densenet_quantized.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPxEUqUMgeBC"
   },
   "source": [
    "Ou loader un modèle déjà existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "e_7pAZEDgeBC",
    "outputId": "1c6606c3-3955-48a1-d9b3-a742195ddedf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_quantized.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_quantized.pt', map_location=torch.device('cpu'))\n",
    "model_quant.load_state_dict(loaded_cpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWtt8NdggeBD"
   },
   "source": [
    "Une autre méthode consiste à quantizer un modèle déjà performant post-training. Pour cela on modifie le state_dict du modèle pour rajouter les paramètres dont on a besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "7SnDPgFZHfG_",
    "outputId": "2bbd26ed-be56-40ad-bf0c-222d9974b140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_trained.pt', map_location=torch.device('cpu'))\n",
    "loaded_cpt_clone = loaded_cpt.copy()\n",
    "for key in loaded_cpt.keys():\n",
    "    if \"conv\" in key:\n",
    "        if key.startswith(\"conv\") == False:\n",
    "            loaded_cpt_clone[key.replace(\"weight\",\"act_alpha\")] = torch.nn.Parameter(torch.tensor(8.0))\n",
    "            loaded_cpt_clone[key.replace(\"weight\",\"weight_quant.wgt_alpha\")] = Parameter(torch.tensor(3.0))\n",
    "model_quant.load_state_dict(loaded_cpt_clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMhprKFTgeBD"
   },
   "source": [
    "Un fine tuning est nécessaire mais cela est moins long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOYXEGREKP6I",
    "outputId": "ac16cfe7-2bdf-4f1c-a23f-13602c2fe680"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-4) \n",
    "scheduler = MultiStepLR(optimizer, milestones=[90, 110], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZrwyXzAgeBE"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses, train_acc, valid_acc = training(trainloader, validloader, model_quant, criterion, optimizer,120,scheduler,'densenet_quantized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Htr1Monhaoq7",
    "outputId": "51cb4f78-d93f-43ce-c072-8eba6b9b3e42"
   },
   "outputs": [],
   "source": [
    "bit = 4\n",
    "for m in model_quant.modules():\n",
    "    if isinstance(m, QuantConv2d):\n",
    "        m.weight_quant = weight_quantize_fn(w_bit=bit)\n",
    "        m.act_grid = build_power_value(bit)\n",
    "        m.act_alq = act_quantization(bit, m.act_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[ 3.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.2000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 1.8000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[-0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-3.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.8000]]],\n",
      "\n",
      "\n",
      "        [[[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-3.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.2000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-3.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.8000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 3.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 1.2000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 3.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 3.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 3.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-3.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[-1.8000]]]], device='cuda:0', grad_fn=<_pqBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = model_quant.dense1[0].conv1\n",
    "print(m.weight_quant(m.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NocOKuK7geBE"
   },
   "source": [
    "On peut voir que les poids sont bien quantizés sur 4 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5v7x3s9zdiv",
    "outputId": "22d3f633-a3c5-47e1-f170-eb7060287629",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation(model_quant, testloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que notre méthode nous donne de bons résultats (nous n'avons pas cherché à obtenir un modèle quantizé avec une précision parfaite car ce modèle n'est pas nécessaire pour le développement de notre modèle final)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est difficile d'implémenter le calcul du score pour notre modèle quantizé donc nous utilisons un modèle non quantizé mais avec la variable Quantization True pour prendre en compte la quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score flops: 0.06795413525587332 Score Params: 0.007912502297752578\n",
      "Final score: 0.0758666375536259\n"
     ]
    }
   ],
   "source": [
    "model = densenet_cifar()\n",
    "model.to(device=device)\n",
    "flops , params = score(model,quantization = True)\n",
    "print(\"Score flops: {} Score Params: {}\".format(flops,params))\n",
    "print(\"Final score: {}\".format(flops + params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le score Micronet est plus faible que le score original donc il peut être intéressant de combiner le pruning et la quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T579-i8HgeBF"
   },
   "source": [
    "# Combinaison Apot Quantization et Pruning\n",
    "Le principal challenge de notre projet était de combiner Quantization et Pruning. En modifiant la classe QuantConv2D, nous avons réussi et cela nous a permis d'obtenir un score au Micronet Challenge de 0.071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les étapes qui vont suivre vont nous permettre de combiner la quantization et le pruning.  \n",
    "Tout d'abord on définit un modèle quanitzé et on lui applique le prunage (on utilise le modèle pruné et non quantizé auquels on rajoute des paramètres utiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZx7BWdvproQ",
    "outputId": "b4fba681-8d9f-4d3d-fe87-2fd7165eb2c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet_Quant(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (dense1): Sequential(\n",
       "    (0): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (4): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (5): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): Transition_Quant(\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): QuantConv2d(\n",
       "      64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (weight_quant): weight_quantize_fn()\n",
       "    )\n",
       "  )\n",
       "  (dense2): Sequential(\n",
       "    (0): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (4): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (5): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (6): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        80, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (7): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        88, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (8): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (9): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        104, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): Transition_Quant(\n",
       "    (bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): QuantConv2d(\n",
       "      112, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (weight_quant): weight_quantize_fn()\n",
       "    )\n",
       "  )\n",
       "  (dense3): Sequential(\n",
       "    (0): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        80, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (4): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        88, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (5): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (6): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        104, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (7): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        112, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (8): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        120, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (9): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (10): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        136, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (11): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (12): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        152, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (13): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        160, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (14): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        168, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (15): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        176, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (16): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        184, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (17): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (18): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        200, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (19): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans3): Transition_Quant(\n",
       "    (bn): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): QuantConv2d(\n",
       "      216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (weight_quant): weight_quantize_fn()\n",
       "    )\n",
       "  )\n",
       "  (dense4): Sequential(\n",
       "    (0): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        108, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        116, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        124, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        132, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (4): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        140, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (5): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        148, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (6): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        156, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (7): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        164, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (8): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        172, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (9): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        180, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (10): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(188, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        188, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "    (11): Bottleneck_Quant(\n",
       "      (bn1): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): QuantConv2d(\n",
       "        196, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=204, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_pruned = densenet_cifar_quant()\n",
    "model_quant_pruned.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "6hlsDFgipt7r"
   },
   "outputs": [],
   "source": [
    "parameters_to_prune=[]\n",
    "for name, module in model_quant_pruned.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) or isinstance(module, QuantConv2d):\n",
    "        parameters_to_prune.append((module,'weight'))\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CinjaCcap0wl",
    "outputId": "4eba55ac-5410-49b7-e58e-832d8e034c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "loaded_cpt_clone = loaded_cpt.copy()\n",
    "for key in loaded_cpt.keys():\n",
    "  if key.startswith(\"conv1\") == False:\n",
    "      if \"conv\" in key and \"orig\" in key:\n",
    "        loaded_cpt_clone[key.replace(\"weight_orig\",\"act_alpha\")] = torch.nn.Parameter(torch.tensor(8.0))\n",
    "        loaded_cpt_clone[key.replace(\"weight_orig\",\"weight_quant.wgt_alpha\")] = Parameter(torch.tensor(3.0))\n",
    "\n",
    "model_quant_pruned.load_state_dict(loaded_cpt_clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit ensuite définir certains paramètres de la quantization et effectuer un forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS-bLysLp5u7",
    "outputId": "baa33d61-8dba-42de-d52d-2048ac8d0cee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 569.035003\n",
      "\n",
      "test accuracy of plane: 72% (726/1000)\n",
      "test accuracy of car: 20% (201/1000)\n",
      "test accuracy of bird:  0% ( 2/1000)\n",
      "test accuracy of cat:  1% (10/1000)\n",
      "test accuracy of deer:  0% ( 0/1000)\n",
      "test accuracy of dog:  0% ( 0/1000)\n",
      "test accuracy of frog:  0% ( 0/1000)\n",
      "test accuracy of horse:  0% ( 0/1000)\n",
      "test accuracy of ship:  0% ( 0/1000)\n",
      "test accuracy of truck:  0% ( 0/1000)\n",
      "\n",
      "test accuracy (overall): 9.39% (939/10000)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model_quant_pruned.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-4) \n",
    "scheduler = MultiStepLR(optimizer, milestones=[80, 110,130], gamma=0.1)\n",
    "\n",
    "evaluation(model_quant_pruned, testloader, criterion)\n",
    "\n",
    "bit = 4\n",
    "for m in model_quant_pruned.modules():\n",
    "  if isinstance(m, QuantConv2d):\n",
    "    m.weight_quant = weight_quantize_fn(w_bit=bit)\n",
    "    m.act_grid = build_power_value(bit)\n",
    "    m.act_alq = act_quantization(bit, m.act_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "lPOCXeRop7xc",
    "outputId": "06b97f03-dcd8-4e6e-fc83-4dc7ae3df877"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses, train_acc, valid_acc = training(trainloader, validloader, model_quant_pruned, criterion, optimizer,150,scheduler,\"Densenet_pruned_quantized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2iQfTSageBJ"
   },
   "source": [
    "Il est également possible de loader notre modèle final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned_quantized.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned_quantized.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "loaded_cpt_clone = loaded_cpt.copy()\n",
    "for key in loaded_cpt.keys():\n",
    "  if key.startswith(\"conv1\") == False:\n",
    "      if \"conv\" in key and \"orig\" in key:\n",
    "        loaded_cpt_clone[key.replace(\"weight_orig\",\"act_alpha\")] = torch.nn.Parameter(torch.tensor(8.0))\n",
    "        loaded_cpt_clone[key.replace(\"weight_orig\",\"weight_quant.wgt_alpha\")] = Parameter(torch.tensor(3.0))\n",
    "\n",
    "model_quant_pruned.load_state_dict(loaded_cpt_clone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Puf2NPb_lD_-",
    "outputId": "bb8478b1-2a38-4c3e-d68d-323f455a9282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 2.4000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-3.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-2.4000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[-0.9000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.8000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3000]],\n",
      "\n",
      "         [[ 1.2000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.8000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 1.2000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[-1.2000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.9000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.6000]],\n",
      "\n",
      "         [[ 0.6000]],\n",
      "\n",
      "         [[ 0.0000]]]], device='cuda:0', grad_fn=<_pqBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = model_quant_pruned.dense1[0].conv1\n",
    "print(m.weight_quant(m.weight,m.weight_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les poids sont bien prunés et quantizés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxeOX7hYgeBJ",
    "outputId": "4bd7dece-6dce-425c-be1b-4f7535f2a10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.399867\n",
      "\n",
      "test accuracy of plane: 90% (908/1000)\n",
      "test accuracy of car: 96% (960/1000)\n",
      "test accuracy of bird: 85% (855/1000)\n",
      "test accuracy of cat: 76% (767/1000)\n",
      "test accuracy of deer: 92% (922/1000)\n",
      "test accuracy of dog: 87% (877/1000)\n",
      "test accuracy of frog: 92% (925/1000)\n",
      "test accuracy of horse: 91% (914/1000)\n",
      "test accuracy of ship: 94% (944/1000)\n",
      "test accuracy of truck: 93% (937/1000)\n",
      "\n",
      "test accuracy (overall): 90.09% (9009/10000)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model_quant_pruned, testloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut noter que notre modèle atteint un score supérieur au 90% requis. Cependant cela a demandé quelques essais donc nous sommes vraiment à la limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.304136\n",
      "\n",
      "test accuracy of plane: 93% (939/1000)\n",
      "test accuracy of car: 97% (974/1000)\n",
      "test accuracy of bird: 90% (903/1000)\n",
      "test accuracy of cat: 84% (843/1000)\n",
      "test accuracy of deer: 95% (950/1000)\n",
      "test accuracy of dog: 88% (881/1000)\n",
      "test accuracy of frog: 94% (944/1000)\n",
      "test accuracy of horse: 93% (935/1000)\n",
      "test accuracy of ship: 94% (949/1000)\n",
      "test accuracy of truck: 94% (947/1000)\n",
      "\n",
      "test accuracy (overall): 92.65% (9265/10000)\n",
      "Score flops: 0.06795413525587332 Score Params: 0.0028913423904609664\n",
      "Final score: 0.07084547764633428\n"
     ]
    }
   ],
   "source": [
    "model_pruned = densenet_cifar()\n",
    "model_pruned.to(device=device)\n",
    "\n",
    "parameters_to_prune=[]\n",
    "for name, module in model_pruned.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) or isinstance(module, QuantConv2d):\n",
    "        parameters_to_prune.append((module,'weight'))\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt')\n",
    "else:\n",
    "    loaded_cpt=torch.load('models\\\\Densenet_cifar_pruned.pt', map_location=torch.device('cpu'))\n",
    "model_pruned.load_state_dict(loaded_cpt)\n",
    "evaluation(model_pruned, testloader, criterion)\n",
    "\n",
    "\n",
    "# En exécutant cette cellule, nous perdons le mask associé au pruning, mais cela nous permet de pouvoir calculer le score micronet\n",
    "for name, module in model_pruned.named_modules():\n",
    "# prune X % of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "    elif isinstance(module, QuantConv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "flops , params = score(model_pruned,quantization = True)\n",
    "print(\"Score flops: {} Score Params: {}\".format(flops,params))\n",
    "print(\"Final score: {}\".format(flops + params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient finalement en combinant ces deux techniques un score Micronet de 0.071   \n",
    "On aurait pu améliorer notre score en quantizant sur 2 bits ou en effectuant du Structured Pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Ce projet nous as permis d'assimiler de nombreuses notions importantes en Deep Learning tel que la a quantization, le pruning, la factorization ou la distillation.  \n",
    "Notre modèle peut évidemment être encore amélioré en utilisant des techniques comme la distillation ou la factorization.  \n",
    "Si vous voulez plus de détails sur notre projet en général nous vous invitons à regarder le fichier Micronet_Challenge.pdf qui résume nos expériences."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Project_Model_Cifar10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
